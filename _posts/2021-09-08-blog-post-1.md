---
title: 'Some Classifiers with MPC'
date: 2021-09-08
permalink: /posts/2021/09/blog-post-3/
tags:
  - MPC
  - Machine Learning
  - Classifiers
---

安全多方计算下机器学习分类器的讨论

机器学习在分类任务中有很好的表现，不同的分类器在特定的分类任务中的表现具有不同的应用。有五个经典的机器学习任务，包括GMM、随机森林、SVM、XGBoost和朴素贝叶斯。基于不同的数据集，也有不同的分类任务，不同的分类器表现相似。

### GMM Classifier

高斯混合分类模型描述了多个高斯成分密度的加权和，其加权和由一个参数化的概率密度函数表示。每一个高斯模型代表一个类，样本数据分别成为几个高斯模型上的投影，并表出每个分类上出现概率，择取概率最大的类为结果。$M$是混合模型的数量，$w^n_i$是混合权重满足$\sum^M_{i=1}w^n_i = 1$可以描述为
$$
\begin{align*}
    p(\vec{x} \vert \lambda_n ) = \sum\nolimits^M_{i=1}w^n_i p^n_i(\vec{x})
\end{align*}
$$

### SVM Classifier

基本的SVM模型是一个二元分类器，学习函数的输出的正负形式对高维数据进行分类。SVM通过数据进行一系列的变换找出最佳边界条件。
$$
\begin{align*}
    minimize: \, Q(w) = \frac{1}{2} \Vert w \Vert^2  \\
    s.t. \quad \forall (x_i, y_i) \in D \; y_i(w \cdot x_i -b) \geq q
\end{align*}
$$

### 随机森林

由多个决策树组成，决策树从输入向量单独采样的随机向量。随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树。


### [XGBoost分类器](https://www.cnblogs.com/pinard/p/10979808.html)

极限梯度提升算法，提升了树突破自身的计算极限，实现运算快速，性能优秀的工程目标。和传统的梯度提升算法相比，XGBoost进行了许多改进，并且已经被认为是在分类和回归上都拥有超高性能的先进评估器。
$$
\begin{align*}
    y^{(t)}_i = \sum \nolimits ^t_{k=1}f_k(x_i) = y^{(t-1)}_i + f_t(x_i)
\end{align*}
$$

### [朴素贝叶斯分类器](https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8)

朴素贝叶斯网络是有向无环图行成的初级贝叶斯网络，包含已观测和未观测的节点，两种节点假设是相对独立的。
$$
\begin{align*}
    \Pr(c \vert x) = \frac{\Pr(x \vert c) \cdot \Pr(c)}{\Pr(x)}
\end{align*}
$$
对于贝叶斯公式来说：$c$表示分类；$x$是测试样本的特征向量；$\Pr(c \vert x)$是后验概率，即给定测试样本向量$x$，它属于类别$c$的概率；同理$\Pr(x \vert c)$是条件概率，给定一个类别$c$，样本$x$属于类别$c$的概率，$\Pr(c)$是前验概率；$\Pr(x)$是预测的后验概率。预测概率$\Pr(x)$是一个归一化常数，这使得$\Pr(c \vert x)$的值落在区间$[0,1]$的范围内。实际方案中，需要比较不同类别的概率，用来确定样本的最大概率。故概率本身不重要，拿它们进行比较值才有意义。由于分母$\Pr(x)$保持不变，因此为方便可以把公式写成如下$$\Pr(c \vert x) = \Pr(x \vert c) \cdot \Pr(c)$$。若假设相互独立的样本特征为$x = (x_1, \dots, x_d)$，则贝叶斯公式写成$$\Pr(c \vert x) = \Pr(c) \prod \nolimits ^d _{k=1}\Pr(x_k \vert c)$$。
在贝叶斯分类器中，计算时概率的数字通常都是极小的小数，而他们相乘之后的积会更小，这在计算机实现中会导致堆栈下溢，从而模型不准确。因此，这就需要把所有的乘法用对数转换成为加法运算，即：
$$
\begin{align*}
    \log(\Pr(c \vert x)) &= \log(\Pr(c) \prod^d_{k=1}\Pr(x_k \vert c)) \\
    &= \log(\Pr(c)) + \sum^d_{k=1}\log(\Pr(x_k \vert c))
\end{align*}
$$

要执行分类算法，就要计算最大的值。朴素贝叶斯分类器的不同之处在于对$\Pr(x \vert c)$的假设。在高斯朴素贝叶斯中，每个类别相关连续值都按照高斯分布。而对于离散的情况来说，则需要使用伯努利朴素贝叶斯或多项式朴素贝叶斯，前者的特征向量由布尔值表示，其中1代表出现，0代表未出现，后者中向量表示样本特征在样本中出现的频率。不同的场景采用不同的贝叶斯方案。
$$
    \begin{align*}
        \hat{c} = \mathsf{argmax}_c[\log \Pr(c) + \sum^d_{k=1} \log (\Pr(x_k \vert c))]
    \end{align*}
$$

