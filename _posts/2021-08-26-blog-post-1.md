---
title: 'Reading Falcon Note2'
date: 2021-08-26
permalink: /posts/2021/08/blog-post-5/
tags:
  - Falcon
  - MPC
  - Secure Comparison
---

处理求数据池中最大值和批量正则化

##### 5. 最大池化及取导
最大池化的作用是输入秘密共享向量，输出最大的共享值。对最大赤化取导，需要输入相同大小的独热([one-hot](https://zh.wikipedia.org/zh-hans/One-hot))向量，1所在的位置是最大值的索引。利用$ \mathsf{ReLU} $协议执行比较，实现最大池化对输入向量的二进制排序。
![](/images/falcon/falcon_maxpool.png)


##### 6. 除法和批量正则化
除法出现在截断操作，目的是丢弃秘密共享的低位，即截断共享的$k$比特$a \rightarrow a/2^k $。处理秘密共享的除法操作有顺序比较和数值计算方法。Falcon文中采用数值方法提高协议效率。数值方法是寻找一个初始化值计算共享除法，它需要估计这个值的范围(算法5)。
![算法5](/images/falcon/faclon_bond.png)
另外文章用算法6计算秘密共享$a,b$的除法$a/b$。通过数值估计方法是得除法转换成多项式，再进行乘法操作。算法先限定$b$的范围$b \rightarrow x,\; x \in [0.5, 1)$。
若$b$是定点数，在算法中计算，$x$的定点数精度为$\alpha +1 $，其中$ 2\alpha \leq b < 2\alpha +1$。令$ w_0 = 2.9142-2x, \epsilon_0 = 1-x\cdot w_0$，初始化近似$ 1 / x = w_0 \cdot (1+ \epsilon_0) $，更高阶的近似，利用泰勒展开求二阶导。每迭代一次通信复杂度都翻倍。
$$ \mathsf{AppDiv}(x) = w_0 \cdot (1+ \epsilon_0)(1+\epsilon_1) \approx \frac{1}{x} $$
![算法6](/images/falcon/faclon_division.png)
批量正则化主要用在神经网络，促进迭代训练提高模型的收敛。算法7的第3步用牛顿方法([Newton's Method](https://zh.wikipedia.org/wiki/%E7%89%9B%E9%A1%BF%E6%B3%95))近似求解，求得$ 1/\sqrt{\sigma^2 + \epsilon} $的近似为$2^{-\left \lfloor \alpha/2 \right \rceil}$，这里$ 2^\alpha \leq \sigma^2 + \epsilon < 2^{\alpha + 1} $，用一个连续的迭代公式：
$$ x_{n+1} = \frac{x_n}{2}(3 - ax^2_n) $$
鉴于当初猜测，经历四轮通信能够满足定点数精度下的近似求解。通过依次计算$ \sqrt{\sigma^2 + \epsilon} $然后再求其逆来得到训练中的批量正则化。在后向传播中，计算$ \sqrt{\sigma^2 + \epsilon} $用做优化网络的方法。用牛顿法计算$$ x_{n+1} = (x_n + a/x_n)/2$$来求$a$的平方根。
![算法7](/images/falcon/falcon_bn.png)



